# Test 2: Deep Multi-Source Analysis

## Test Objective

Verify the skill correctly performs comprehensive reconnaissance with multiple sources, cross-referencing, and conflict resolution.

## Test Input

```yaml
target: "vector databases"
questions:
  - "Compare Pinecone, Weaviate, and Milvus"
  - "Which is best for production at scale?"
  - "What are the cost implications?"
depth: "full"
constraints:
  time_limit: "6 hours"
  focus: "enterprise production use"
```

## Expected Behavior

### Phase 1: Scope Definition
- [ ] Multi-target comparison correctly identified
- [ ] Constraints include: enterprise, production, cost
- [ ] Success criteria defined for comparison

### Phase 2: Source Discovery
For EACH target (Pinecone, Weaviate, Milvus):
- [ ] GitHub repository (where applicable)
- [ ] Official documentation
- [ ] Pricing page
- [ ] At least one independent benchmark/review

Total sources expected: >= 12

### Phase 3: Extraction
- [ ] Per-target MANIFEST.md created
- [ ] Feature extraction normalized across targets
- [ ] Pricing models documented
- [ ] Performance claims captured with sources

### Phase 4: Synthesis
- [ ] COMPARISON-CHART.md with:
  - Feature matrix
  - Pricing comparison
  - Performance benchmarks
  - Deployment complexity
- [ ] Conflicts identified (e.g., benchmark discrepancies)
- [ ] Gaps documented (missing information)
- [ ] Clear recommendation with rationale

### Phase 5: Delivery
- [ ] Full reconnaissance package
- [ ] Memory-mcp entries created
- [ ] Executive summary for stakeholders
- [ ] Confidence assessment per claim

## Success Criteria

| Criterion | Threshold | Pass/Fail |
|-----------|-----------|-----------|
| Sources per target | >= 4 | |
| Cross-reference matrix | Completed | |
| Conflicts surfaced | >= 1 identified | |
| Pricing clarity | All 3 documented | |
| Recommendation strength | Specific with caveats | |
| Confidence calibration | Varies by claim type | |

## Quality Gates

### Evidence Tracking
- [ ] Every feature claim has source link
- [ ] Benchmark claims cite specific tests
- [ ] Pricing claims have date and tier info

### Conflict Handling
- [ ] Conflicting benchmarks noted
- [ ] Resolution or investigation proposed
- [ ] User warned of uncertainty

### Completeness
- [ ] No target has < 3 sources
- [ ] All questions answered
- [ ] Gaps explicitly listed

## Anti-Pattern Detection

- [ ] NOT treating marketing claims as facts
- [ ] NOT missing any of the 3 targets
- [ ] NOT providing single-vendor recommendation without caveats
- [ ] NOT extrapolating pricing without tier clarity

## Stress Test Variations

1. **Conflicting Sources:** Inject deliberately conflicting benchmark data
2. **Missing Pricing:** One vendor has no public pricing
3. **Stale Source:** Include an outdated comparison article

Expected handling:
- Conflicts noted and flagged
- Missing data marked as gap
- Stale sources deprioritized or excluded

## Notes

This test validates the skill's ability to handle realistic enterprise evaluation scenarios where information is imperfect and trade-offs must be communicated clearly.
