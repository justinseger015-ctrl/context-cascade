# Example 3: Paper Extraction

## Scenario

User needs to extract implementation patterns from federated learning papers for fog-compute integration.

## Input

```yaml
target: "federated learning papers"
sources:
  - "arXiv:2504.00407"  # AMP4EC
  - "arXiv:2411.16086"  # HiDP
  - "arXiv:2503.18986"  # SplitFrozen
questions:
  - "What are the key algorithms?"
  - "Is code available?"
  - "What are the performance claims?"
depth: "full"
```

## Execution Trace

### Phase 1: Scope Definition
```
Output:
  understood_intent: "Extract implementable patterns for integration"
  constraints:
    - HARD: Focus on edge/fog inference patterns
    - HARD: Need actual algorithms, not just concepts
    - SOFT: Code availability preferred
    - INFERRED: Integration with NSGA-II scheduler
```

### Phase 2: Source Retrieval
```
Papers retrieved via arXiv API:
1. AMP4EC (FMEC 2025) - Adaptive Model Partitioning
2. HiDP (DATE 2025) - Hierarchical DNN Partitioning
3. SplitFrozen (Preprint) - LLM Fine-tuning
```

### Phase 3: Per-Paper Extraction

#### Paper 1: AMP4EC
```markdown
# AMP4EC Extraction

**Full Title:** Adaptive Model Partitioning for Edge Computing
**Venue:** FMEC 2025 (10th International Conference on Fog and Mobile Edge Computing)
**Authors:** Zhang, Guo, Tan, Jiang

## Key Algorithm
- Resource-aware DNN partitioning
- Weighted scoring for device selection
- Docker-based deployment

## Architecture Components
| Component | Function | Integration Point |
|-----------|----------|-------------------|
| Resource Monitor | Real-time device tracking | FogCoordinator |
| Model Partitioner | Device-aware splitting | NSGA-II objective |
| Task Scheduler | Weighted load balancing | Existing scheduler |

## Performance Claims
- 78% latency reduction (witnessed in paper)
- 414% throughput improvement (witnessed in paper)

## Code Availability
- NO public repository found
- Conceptual implementation provided in paper

## Integration Effort
Estimated 12 hours to implement core pattern
```

#### Paper 2: HiDP
```markdown
# HiDP Extraction

**Full Title:** Hierarchical DNN Partitioning for Heterogeneous Edge
**Venue:** IEEE DATE 2025

## Key Algorithm
- Two-level partitioning: Global + Local
- FSM-based scheduler (5 states)
- Core-level heterogeneity awareness

## FSM States
ANALYZE -> EXPLORE -> OFFLOAD -> MAP -> EXECUTE

## Performance Claims
- 38% avg latency reduction
- 65% max latency reduction
- 46% energy reduction

## Code Availability
- NO public repository found

## Integration Effort
Estimated 16 hours (FSM + two-level logic)
```

#### Paper 3: SplitFrozen
```markdown
# SplitFrozen Extraction

**Full Title:** Efficient LLM Fine-tuning on Heterogeneous Devices
**Date:** March 2025 (Preprint)

## Key Innovation
- Frozen device-side layers (forward only)
- 87% compute reduction on devices
- Activation shuffle for non-IID resilience

## Three Components
1. Heterogeneity-aware partitioning
2. Non-IID resilience via shuffle
3. Pipeline parallelism

## Performance Claims
- 69.4% better than FedLoRA (extreme non-IID)
- 86.8% device compute reduction
- 50.2% training time reduction

## Code Availability
- NO public repository found

## Integration Effort
Estimated 20 hours (requires FATE-LLM base)
```

### Phase 4: Synthesis

```markdown
# FL-COMPREHENSIVE-COMPARISON-CHART.md

| Aspect | AMP4EC | HiDP | SplitFrozen |
|--------|--------|------|-------------|
| Focus | Inference | Inference | Training |
| Partitioning | Device-aware | Global+Local | Frozen layers |
| Scheduling | Weighted | FSM | Pipeline |
| Latency Reduction | 78% | 65% | N/A |
| Code Available | NO | NO | NO |
| Integration Effort | 12h | 16h | 20h |

## Integration Roadmap

### Phase 1 (Week 1): AMP4EC Pattern
- Add resource profiling to FogCoordinator
- Implement weighted scoring
- Add partition_efficiency to NSGA-II

### Phase 2 (Week 2): HiDP Pattern
- Implement FSM scheduler
- Add global/local partitioning
- Core-level mapping

### Phase 3 (Week 3): SplitFrozen Pattern
- Frozen layer configuration
- Activation shuffle
- Pipeline parallelism

Total: 48 hours implementation
```

## Output

Confidence: 0.82 (ceiling: research 0.85)

Rationale: Direct paper analysis (witnessed), performance claims from papers (reported with paper as source), integration estimates (inferred from algorithm complexity).
