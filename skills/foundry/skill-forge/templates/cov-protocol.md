# Chain-of-Verification (CoV) Protocol

## Kanitsal Cerceve (Evidential Frame Activation)
Kaynak dogrulama modu etkin.



**Research Foundation**: Dhuliawala et al. (2023) - 42% error reduction, 37% improvement in completeness

**Purpose**: Systematic self-critique mechanism to catch errors, gaps, and ambiguities before they propagate through the skill creation process.

---

## When to Apply CoV

Use Chain-of-Verification whenever:
- Making critical design decisions
- Analyzing requirements or intent
- Writing instructions or specifications
- Making claims that need verification
- Completing any phase with high error cost if wrong

---

## The 5-Step CoV Process

### Step 1: Generate Initial Output

**Action**: Create your initial analysis, design, or content without self-censoring.

**Example** (Phase 1 - Intent Analysis):
```
Initial Understanding:
- User wants a skill to format code
- Will use Prettier for JavaScript
- Should support multiple file types
- Needs to integrate with existing workflow
```

---

### Step 2: Self-Critique

**Action**: Ask "What might be wrong, incomplete, or misleading about what I just created?"

**Critical Questions**:
- What assumptions did I make?
- What did I miss or overlook?
- Where might I have misunderstood?
- What edge cases exist?
- What could fail or go wrong?

**Example**:
```
Self-Critique:
- Assumption: User wants Prettier specifically (not verified)
- Missing: What file types exactly? All languages or specific ones?
- Overlooked: Does "integrate with workflow" mean automated or manual?
- Edge case: What if files are already formatted? What if format conflicts with team style?
- Failure mode: What if Prettier not installed? What if unsupported file type?
```

---

### Step 3: Evidence Check

**Action**: For each claim or decision, cite evidence FOR and AGAINST.

**Format**:
```
Claim/Decision: [statement]
Evidence FOR:
  - [supporting fact/requirement]
  - [supporting fact/requirement]
Evidence AGAINST:
  - [contradicting fact/edge case]
  - [alternative interpretation]
```

**Example**:
```
Claim: User wants Prettier for JavaScript
Evidence FOR:
  - User mentioned "format code"
  - Prettier is popular formatter
Evidence AGAINST:
  - User didn't specify Prettier explicitly
  - Could want Black (Python), rustfmt (Rust), etc.
  - "Multiple file types" suggests not JS-only

Decision: Need to clarify - don't assume Prettier
```

---

### Step 4: Revise Based on Critique

**Action**: Update your initial output to address gaps, correct errors, and resolve ambiguities identified in Steps 2-3.

**Example**:
```
Revised Understanding:
- User wants a CODE FORMATTING skill (tool unspecified)
- File types: Need to clarify (JS? Python? Multiple?)
- Integration: Need to clarify (manual trigger? auto-format on save?)
- Edge cases to handle:
  - Already formatted files (skip or re-format?)
  - Unsupported file types (error or skip?)
  - Tool not installed (install automatically or error?)

Strategic Questions to Ask:
1. Which languages/file types need formatting?
2. Which formatter per language (Prettier/Black/rustfmt/etc.)?
3. Manual trigger or automatic?
4. How to handle missing formatters?
```

---

### Step 5: Confidence Rating

**Action**: Rate confidence level for each component of your revised output.

**Confidence Scale**:
- **High (H)**: Verified with evidence, minimal uncertainty
- **Medium (M)**: Reasonable inference, some uncertainty
- **Low (L)**: Assumption-based, needs validation

**Example**:
```
Confidence Ratings:
- User wants formatting skill: HIGH (explicitly stated)
- Multiple file types: MEDIUM (implied by "multiple file types")
- Tool choice matters: HIGH (different tools for different languages)
- Integration approach: LOW (not specified, needs clarification)
- Edge case handling: LOW (not discussed, needs design)

Overall: MEDIUM confidence - need user clarification on 3/5 aspects
```

---

## CoV Templates by Phase

### Phase 1: Intent Archaeology CoV

```markdown
## Phase 1b: Intent Verification (CoV)

**Step 1 - Initial Understanding**: [Document what you understood]

**Step 2 - Self-Critique**:
- What assumptions did I make? [List]
- What might I have misunderstood? [List]
- What's missing or unclear? [List]
- What edge cases exist? [List]

**Step 3 - Evidence Check**:
For each key requirement:
- Claim: [requirement]
- Evidence FOR: [supporting facts]
- Evidence AGAINST: [contradicting facts]
- Resolution: [how to resolve]

**Step 4 - Revised Understanding**:
- Primary use cases: [updated list]
- Key requirements: [updated list]
- Important constraints: [updated list]
- Success criteria: [updated list]
- Strategic questions: [what needs clarification]

**Step 5 - Confidence Ratings**:
- [Requirement 1]: [H/M/L] - [why]
- [Requirement 2]: [H/M/L] - [why]
- Overall: [H/M/L] - [summary]

**Quality Gate**: Do NOT proceed to Phase 2 if overall confidence is LOW. Seek clarification first.
```

---

### Phase 5: Instruction Crafting CoV

```markdown
## Phase 5b: Instruction Verification (CoV)

**Step 1 - Initial Instructions**: [Already written in Phase 5]

**Step 2 - Self-Critique**:
- Ambiguous instructions: [Which instructions could be misinterpreted?]
- Missing steps: [What did I forget to include?]
- Unclear success criteria: [Where is "done" ambiguous?]
- Edge cases not handled: [What could go wrong?]
- Known anti-patterns: [Vague verbs? Missing examples?]

**Step 3 - Evidence Check**:
Test each instruction for ambiguity:
- Instruction: [step]
- Can this be misinterpreted? [Yes/No - how?]
- Is success criteria clear? [Yes/No - what's missing?]
- Are edge cases handled? [Yes/No - which missing?]

**Step 4 - Revised Instructions**:
For each ambiguous/incomplete instruction:
- Original: [vague instruction]
- Revised: [clear, explicit instruction with success criteria]
- Edge cases: [how to handle them]

**Step 5 - Confidence Ratings**:
Per instruction section:
- [Section 1]: [H/M/L] - [rationale]
- [Section 2]: [H/M/L] - [rationale]
Overall: [H/M/L]

**Quality Gate**: Do NOT proceed to Phase 6 if:
- Any section has LOW confidence
- >20% of instructions lack explicit success criteria
- Any known anti-patterns detected
```

---

## Adversarial Self-Testing (CoV Enhancement)

After completing CoV, actively try to BREAK your analysis/design:

1. **Intentional Misinterpretation**: Follow instructions in bad faith - can you generate wrong results while technically following them?
2. **Remove Prerequisites**: What if expected tools/files missing?
3. **Boundary Testing**: Test with edge cases (empty input, huge input, malformed input)
4. **Alternative Interpretations**: Can instructions be read a different way?

If any attacks succeed, return to Step 4 (Revise).

---

## Common CoV Pitfalls

### ❌ Pitfall 1: Superficial Self-Critique
**Bad**: "This looks good, no issues found"
**Good**: "Assumption: user wants X. Evidence FOR: []. Evidence AGAINST: []. Need to verify."

### ❌ Pitfall 2: Ignoring Low-Confidence Items
**Bad**: Proceeding despite multiple LOW confidence ratings
**Good**: Seeking clarification when overall confidence is LOW

### ❌ Pitfall 3: No Concrete Evidence
**Bad**: "I think this is right"
**Good**: "Based on requirement stated in line 23, this maps to..."

### ❌ Pitfall 4: Skipping Revision
**Bad**: Running CoV but not updating original output
**Good**: Creating revised version addressing all critique points

### ❌ Pitfall 5: Vague Confidence Ratings
**Bad**: "Seems okay, probably fine"
**Good**: "HIGH confidence - verified against 3 requirements. LOW confidence - assumption not validated."

---

## Benefits of CoV (Research-Backed)

| Metric | Without CoV | With CoV | Improvement |
|--------|-------------|----------|-------------|
| **Factual Errors** | Baseline | -42% | 42% reduction |
| **Completeness** | Baseline | +37% | 37% more complete |
| **Ambiguity** | Baseline | -35% | 35% clearer |
| **First-Time-Right** | 40% | 85% | +113% |

**Source**: Dhuliawala et al. (2023) - "Chain-of-Verification Reduces Hallucination in Large Language Models"

---

## Integration with Other Techniques

CoV works synergistically with:
- **Adversarial Testing**: CoV identifies weaknesses, adversarial testing exploits them
- **Multi-Persona Debate**: Each persona applies CoV to their analysis
- **Quality Gates**: CoV provides pass/fail criteria for gates
- **Metrics Tracking**: Confidence ratings become quantitative metrics

---

## Quick Reference Checklist

For any critical decision/analysis:
- ✓ Step 1: Generate initial output
- ✓ Step 2: Self-critique (assumptions, gaps, edge cases)
- ✓ Step 3: Evidence FOR and AGAINST each claim
- ✓ Step 4: Revise based on critique
- ✓ Step 5: Rate confidence (H/M/L per component)
- ✓ Quality Gate: Don't proceed if overall confidence LOW

**Time Investment**: 5-10 minutes per phase
**ROI**: 42% error reduction, 37% completeness improvement
**When to Use**: Phases 1, 2, 3, 5 (critical design points)

---

**Remember**: CoV is not about being perfect upfront - it's about catching errors through systematic self-critique BEFORE they become expensive to fix later.


---
*Promise: `<promise>COV_PROTOCOL_VERIX_COMPLIANT</promise>`*
