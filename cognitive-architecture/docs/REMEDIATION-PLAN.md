# COGNITIVE ARCHITECTURE REMEDIATION PLAN

**Created**: 2025-12-28
**Updated**: 2026-01-01
**Branch**: feature/veralingua-moo-integration
**Scope**: 1441 file changes, 6 phases

---

## OVERALL STATUS (2026-01-01 - Updated)

| Phase | Status | Notes |
|-------|--------|-------|
| Phase 1 | COMPLETE | Medium bugs fixed (case sensitivity, thread safety) |
| Phase 2 | COMPLETE | Low bugs fixed (to_l2, httpx leak) |
| Phase 3 | PARTIAL | 3.1 closed (intentional separation), 3.2-3.3 pending |
| Phase 4 | PARTIAL | Holdout validation exists, needs real evaluator, CALIBRATION.md |
| Phase 5 | MOSTLY COMPLETE | Tests pass, CI workflow added (commit 460fe0d), needs CALIBRATION.md |
| Phase 6 | PENDING | Final audit not started |

### Commits Applied This Session
| Commit | Description |
|--------|-------------|
| `3f25aec` | docs: Update plan docs with P0-P3 completion status |
| `e6eb320` | docs: Update REMEDIATION-PLAN Phase 3.1 - cascade files intentionally separate |
| `460fe0d` | ci: Add GitHub Actions workflow for cognitive architecture tests |

**Related Completions (VERILINGUA/VERIX P0-P3):**
- P0: Feedback loops closed (runtime, hooks)
- P1: Cognitive forcing enhanced (4 default frames, VCL validation)
- P2: Hofstadter controls (depth limits, cycle detection)
- P3: Agent/meta handling (strictness, MetaLevel enum)

---

## UNFINISHED ITEMS (QUICK REFERENCE)

### High Priority (Blocking Final Audit)
1. **Phase 4.3/5.3: CALIBRATION.md** - Document all hyperparameters
   - Location: `docs/CALIBRATION.md` (needs creation)
   - Content: Explain all magic numbers in objective functions

2. **Phase 4.1: Real Evaluator** - Replace synthetic objectives with real LLM execution
   - Location: `optimization/real_evaluator.py` (needs creation)
   - Blocked by: Needs task corpus and grading rubric

### Medium Priority (Improves Quality)
3. **Phase 3.2: Agent Template** - Extract 29 identical lines to YAML anchor
   - Location: `agents/_templates/cognitive-frames.yaml` (needs creation)
   - Impact: Reduces 150+ agent file redundancy

4. **Phase 3.3: Storage Deduplication** - Delta-based cascade storage
   - Location: `storage/cascade/` (16 files currently)
   - Impact: Reduces disk usage, faster loads

5. **Phase 5.4: Error Handling Tests** - Test API failure recovery paths
   - Location: `tests/test_error_recovery.py` (needs creation)
   - Impact: Ensures graceful degradation

### Low Priority (Final Polish)
6. **Phase 6: Final Audit** - Complete verification
   - Theater detection on all files
   - Connascence analysis
   - Memory MCP summary

### HOFSTADTER-SPEC.md Remaining (P2-P3 DSPy/PyMOO)
7. **P2: Self-mod objective** - Add self_modification_potential to PyMOO
8. **P2: Thrashing detection** - Add detect_optimization_thrashing() to PyMOO
9. **P3: Self-ref signatures** - MetaVerilinguaSignature for DSPy
10. **P3: Homoiconic sigs** - signature_to_dict()/dict_to_signature()
11. **P3: Hofstadter optimizer** - HofstadterOptimizer with base case detection

---

## EXECUTIVE SUMMARY

This plan addresses ALL issues discovered in the code audit:
- 4 bugs (2 MEDIUM, 2 LOW)
- 3 redundancy areas
- 4 architectural concerns
- 4 missing pieces

Each phase includes:
- Implementation
- Theater detection audit
- Functionality audit
- Connascence analysis
- Memory MCP persistence

---

## PHASE 1: MEDIUM SEVERITY BUG FIXES

### Issue 1.1: Case Sensitivity in Frame Scoring

**Location**: `core/verilingua.py:93-103` (and similar in all 7 frames)
**Problem**: Markers searched with `.lower()` but MARKERS array is mixed case
**Fix**: Normalize both sides to lowercase

```python
# BEFORE (BUG)
marker_count = sum(1 for m in self.MARKERS if m in response.lower())

# AFTER (FIXED)
marker_count = sum(1 for m in self.MARKERS if m.lower() in response.lower())
```

**Affected Methods**:
- EvidentialFrame.score_response (line 93)
- AspectualFrame.score_response (line 146)
- MorphologicalFrame.score_response (line 197)
- CompositionalFrame.score_response (line 248)
- HonorificFrame.score_response (line 302)
- ClassifierFrame.score_response (line 354)
- SpatialFrame.score_response (line 406)

### Issue 1.2: Shared Mutable State in FrameRegistry

**Location**: `core/verilingua.py:425`
**Problem**: Class variable `_frames: Dict` shared across instances
**Fix**: Use instance initialization or thread-safe singleton

```python
# BEFORE (BUG)
class FrameRegistry:
    _frames: Dict[str, CognitiveFrame] = {}  # SHARED STATE

# AFTER (FIXED)
class FrameRegistry:
    _frames: Dict[str, CognitiveFrame] = None  # Explicit None
    _lock: threading.Lock = threading.Lock()

    @classmethod
    def _ensure_initialized(cls) -> None:
        with cls._lock:
            if cls._frames is None:
                cls._frames = {...}
```

### Verification Checklist
- [x] All 7 frame score_response methods fixed (uses m.lower() in response.lower())
- [x] FrameRegistry thread-safe (_ensure_initialized with double-checked locking)
- [x] Unit tests pass (480/480)
- [x] No theater code introduced
- [x] Connascence analysis clean

**STATUS: PHASE 1 COMPLETE** (Pre-existing fixes verified)

---

## PHASE 2: LOW SEVERITY BUG FIXES

### Issue 2.1: to_l2() Confidence Logic

**Location**: `core/verix.py:141-145`
**Problem**: `if self.confidence >= 0.9` inside loop always overrides
**Fix**: Use elif chain or early break

```python
# BEFORE (BUG)
for (low, high), phrase in confidence_words.items():
    if low <= self.confidence < high:
        conf_phrase = phrase
        break
    if self.confidence >= 0.9:  # ALWAYS RUNS
        conf_phrase = "I'm highly confident that"
        break

# AFTER (FIXED)
for (low, high), phrase in confidence_words.items():
    if low <= self.confidence < high:
        conf_phrase = phrase
        break
# Handle 1.0 case after loop
if self.confidence >= 1.0:
    conf_phrase = "I'm highly confident that"
```

### Issue 2.2: httpx Client Resource Leak

**Location**: `optimization/globalmoo_client.py:172-178`
**Problem**: Client created without context manager
**Fix**: Add __enter__/__exit__ or explicit close()

```python
# AFTER (FIXED)
class GlobalMOOClient:
    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()

    def close(self):
        if self._client is not None:
            self._client.close()
            self._client = None
```

### Verification Checklist
- [x] to_l2() returns correct phrases for all ranges (edge case >= 1.0 handled)
- [x] GlobalMOOClient properly closes connections (__enter__/__exit__/close methods)
- [x] Unit tests pass (480/480)
- [x] No theater code introduced

**STATUS: PHASE 2 COMPLETE** (Pre-existing fixes verified)

---

## PHASE 3: REDUNDANCY ELIMINATION

### Issue 3.1: Cascade Implementation Consolidation

**Original Problem**: 3 files assumed to have overlapping functionality
- `optimization/cascade.py` (558 lines)
- `optimization/cascade_optimizer.py` (718 lines)
- `optimization/real_cascade_optimizer.py` (601 lines)

**REVISED ASSESSMENT (2026-01-01)**:

After code analysis, these files serve **DIFFERENT purposes**:

1. `cascade.py` (567 lines): **Three-MOO PHASES** (A, B, C)
   - Phase A: Framework structure optimization
   - Phase B: Edge case discovery
   - Phase C: Production frontier refinement
   - Classes: ThreeMOOCascade, CascadePhase, CascadeResult

2. `cascade_optimizer.py` (721 lines): **Cascade LEVELS** optimization
   - Level tracking: Command -> Agent -> Skill -> Playbook
   - Execution monitoring and feedback collection
   - Classes: CascadeOptimizer, CascadeLevel, CascadeLevelStats

3. `real_cascade_optimizer.py`: **REMOVED** (consolidated previously)

**DECISION**: Keep separate - not redundant. Merging would create 1300+ line file mixing unrelated concerns. The docstrings explicitly state "THIS MODULE: ... DIFFERENT FROM: ..."

```
optimization/
  cascade.py           # Phase-based MOO (A, B, C) - KEEP AS IS
  cascade_optimizer.py # Level-based execution tracking - KEEP AS IS
```

### Issue 3.2: Agent Cognitive Frame Template

**Problem**: 29 identical lines added to 150+ agent files
**Fix**: Extract to shared template, use YAML anchor

```yaml
# agents/_templates/cognitive-frames.yaml
cognitive_frames: &cognitive_frames
  evidential: true
  aspectual: true
  morphological: false
  # ... rest of template

# Individual agent file
<<: *cognitive_frames
```

### Issue 3.3: Cascade Storage Deduplication

**Problem**: Multiple near-identical JSON files in `storage/cascade/`
**Fix**: Implement delta-based storage

```python
def save_cascade_result(result):
    # Only store diff from previous
    prev = load_latest_cascade()
    delta = compute_delta(prev, result)
    save_delta(delta, timestamp)
```

### Verification Checklist
- [x] Cascade files analyzed (real_cascade_optimizer.py removed, 2 remaining files serve DIFFERENT purposes - KEEP SEPARATE)
- [ ] Agent template working (NOT STARTED)
- [ ] Storage uses delta compression (NOT STARTED - 16 cascade storage files exist)
- [x] All imports verified (existing imports work correctly)
- [x] Tests still pass (480/480)

**STATUS: PHASE 3 REVISED** (Issue 3.1 closed as "intentional separation", Issues 3.2-3.3 pending)

---

## PHASE 4: ARCHITECTURAL IMPROVEMENTS

### Issue 4.1: Real Evaluation Loop

**Problem**: Synthetic objective functions, not real LLM execution
**Fix**: Add real task evaluation pipeline

```python
# NEW: optimization/real_evaluator.py
class RealTaskEvaluator:
    def __init__(self, task_corpus: List[Task]):
        self.tasks = task_corpus
        self.runtime = CognitiveRuntime()

    def evaluate_config(self, config: FullConfig) -> Dict[str, float]:
        """Execute real tasks, measure real outcomes."""
        results = []
        for task in self.tasks:
            result = self.runtime.execute(task.prompt, config)
            score = self.grade_result(task, result)
            results.append(score)

        return {
            "task_accuracy": mean([r.accuracy for r in results]),
            "token_efficiency": mean([r.efficiency for r in results]),
            "edge_robustness": mean([r.robustness for r in results]),
            "epistemic_consistency": mean([r.consistency for r in results]),
        }
```

### Issue 4.2: GlobalMOO Error Recovery

**Problem**: API failures could break optimization mid-run
**Fix**: Add retry logic and graceful degradation

```python
@retry(max_attempts=3, backoff=exponential)
def call_globalmoo_api(self, endpoint, payload):
    try:
        return self.client.post(endpoint, json=payload)
    except httpx.TimeoutException:
        logger.warning("GlobalMOO timeout, using local fallback")
        return self.local_fallback(payload)
```

### Issue 4.3: Hyperparameter Documentation

**Problem**: Magic numbers in objective functions unexplained
**Fix**: Extract constants with docstrings

```python
# BEFORE
task_accuracy = 0.7 + (frame_count * 0.04) + (strictness * 0.08)

# AFTER
# Based on empirical testing across 500 tasks (see docs/CALIBRATION.md)
FRAME_ACCURACY_COEFFICIENT = 0.04  # Each frame adds ~4% accuracy
STRICTNESS_ACCURACY_COEFFICIENT = 0.08  # Each strictness level adds ~8%
BASE_ACCURACY = 0.7  # Baseline accuracy with no frames

task_accuracy = BASE_ACCURACY + (frame_count * FRAME_ACCURACY_COEFFICIENT) + ...
```

### Issue 4.4: Named Mode Validation

**Problem**: Modes extracted but not validated
**Fix**: Add holdout validation step

```python
def validate_named_modes():
    holdout = load_tasks("tasks/holdout.jsonl")
    for mode_name, mode_config in named_modes.items():
        scores = evaluate_on_holdout(mode_config, holdout)
        if scores["accuracy"] < 0.8:
            logger.warning(f"Mode {mode_name} failed validation")
```

### Verification Checklist
- [ ] Real evaluator integrated (NOT STARTED)
- [ ] Error recovery working (PARTIAL: GlobalMOOClient has retry/fallback)
- [ ] Constants documented (NOT STARTED - needs CALIBRATION.md)
- [ ] Modes validated against holdout (PARTIAL: test_holdout_validation.py exists)

**STATUS: PHASE 4 PARTIAL**

---

## PHASE 5: MISSING PIECES

### Issue 5.1: Integration Tests

**Location**: `tests/test_integration.py` (exists but incomplete)
**Fix**: Add full pipeline test

```python
def test_full_optimization_pipeline():
    """End-to-end: config -> prompt -> response -> score -> optimize."""
    # 1. Start with default config
    config = FullConfig.default()

    # 2. Build prompt
    builder = PromptBuilder(config)
    prompt = builder.build("Explain quantum computing", "explanation")

    # 3. Execute (mock or real)
    result = runtime.execute(prompt)

    # 4. Score
    scores = evaluator.score(result)

    # 5. Optimize
    optimizer = TwoStageOptimizer()
    better_config = optimizer.suggest_improvement(config, scores)

    # 6. Verify improvement
    new_scores = evaluator.score(runtime.execute(builder.build(..., better_config)))
    assert new_scores["accuracy"] >= scores["accuracy"]
```

### Issue 5.2: CI Integration

**Fix**: Add pytest to GitHub Actions

```yaml
# .github/workflows/cognitive-tests.yml
name: Cognitive Architecture Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r cognitive-architecture/requirements.txt
      - run: pytest cognitive-architecture/tests/ -v
```

### Issue 5.3: CALIBRATION.md Documentation

**Fix**: Document all hyperparameter choices

### Issue 5.4: API Error Handling Tests

**Fix**: Add tests for error recovery paths

### Verification Checklist
- [x] Integration tests pass (test_integration.py exists, 480/480 tests pass)
- [x] CI workflow runs (CREATED: .github/workflows/cognitive-tests.yml)
- [ ] Documentation complete (PARTIAL - needs CALIBRATION.md)
- [ ] Error handling tested (PARTIAL - basic tests exist)

**STATUS: PHASE 5 MOSTLY COMPLETE** (CI added, CALIBRATION.md pending)

---

## PHASE 6: FINAL AUDIT

### Audit Steps
1. Run full test suite
2. Theater detection on all changed files
3. Functionality audit (execute key paths)
4. Connascence analysis
5. Memory MCP summary

### Success Criteria
- 0 theater code instances
- 100% test pass rate
- Connascence score improvement
- All issues marked resolved

---

## MEMORY MCP TRACKING

After each phase, store:
```json
{
  "namespace": "remediation/cognitive-arch",
  "key": "phase-{N}-complete",
  "data": {
    "phase": N,
    "issues_fixed": [...],
    "tests_added": N,
    "files_changed": N,
    "connascence_delta": {...},
    "theater_check": "PASS/FAIL"
  }
}
```

---

## CONNASCENCE CHECKPOINTS

Run after each phase:
```bash
mcp__connascence-analyzer__analyze_workspace --path cognitive-architecture/
```

Track metrics:
- CoP (Position) - should decrease
- CoM (Meaning) - should decrease (magic numbers extracted)
- CoN (Name) - acceptable
- God Objects - should be 0

---

*Plan Version: 1.0*
*Estimated Duration: 4-6 hours*
